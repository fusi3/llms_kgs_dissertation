{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WXtzeqzZekqW",
        "kJyNkzNx6k-7",
        "jH-h5Juc2IpC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e32c3f1fd3ec461ba77c440add295b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5c7a704c1324feeb5cd3f9313a9db8c",
              "IPY_MODEL_b6e248aa08444d58976a67228702588b",
              "IPY_MODEL_43ee5e71ea1840f99f2b023ffd8f4a6b"
            ],
            "layout": "IPY_MODEL_5e83e43148f4419a87aee84aedbd92fd"
          }
        },
        "f5c7a704c1324feeb5cd3f9313a9db8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ad9701f5c094bc38e9f62d4d28ffcf3",
            "placeholder": "​",
            "style": "IPY_MODEL_e66e43ef2c3243caba5741528e9a4706",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b6e248aa08444d58976a67228702588b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee93a87dd35147c890651d6955babf2c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_555e4e7706f1406d9a437d69ede203fe",
            "value": 2
          }
        },
        "43ee5e71ea1840f99f2b023ffd8f4a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_291ba42cf14a4f608713758e2983aeff",
            "placeholder": "​",
            "style": "IPY_MODEL_2d9acc46f1984935bdf1bc1c386c608c",
            "value": " 2/2 [01:14&lt;00:00, 34.52s/it]"
          }
        },
        "5e83e43148f4419a87aee84aedbd92fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ad9701f5c094bc38e9f62d4d28ffcf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e66e43ef2c3243caba5741528e9a4706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee93a87dd35147c890651d6955babf2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "555e4e7706f1406d9a437d69ede203fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "291ba42cf14a4f608713758e2983aeff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9acc46f1984935bdf1bc1c386c608c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30fa4943b3794cf1b113af99476e4b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_042ea93e326340a894ca4dd03211eff1",
              "IPY_MODEL_57691a5266704ea7b5bfaaa5aedfe882",
              "IPY_MODEL_994fe9dfb6f24c92a125cc23754971a0"
            ],
            "layout": "IPY_MODEL_8174948df6be4feba0d58677288b7592"
          }
        },
        "042ea93e326340a894ca4dd03211eff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc6a9e94e62241adaaa0ef96d7c89291",
            "placeholder": "​",
            "style": "IPY_MODEL_ca4b6ef438414d52ad38cffaf5a7c4f6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "57691a5266704ea7b5bfaaa5aedfe882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72fafebfe2434fd2847b65d69d0227ae",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cb7c1098e4b41ff97ea22c1066722d1",
            "value": 2
          }
        },
        "994fe9dfb6f24c92a125cc23754971a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e249712556e541a9abe4f513e992f27c",
            "placeholder": "​",
            "style": "IPY_MODEL_a7e88283da6d4df5a9a4a05ea21133e4",
            "value": " 2/2 [00:53&lt;00:00, 24.01s/it]"
          }
        },
        "8174948df6be4feba0d58677288b7592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6a9e94e62241adaaa0ef96d7c89291": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4b6ef438414d52ad38cffaf5a7c4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72fafebfe2434fd2847b65d69d0227ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb7c1098e4b41ff97ea22c1066722d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e249712556e541a9abe4f513e992f27c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e88283da6d4df5a9a4a05ea21133e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "JovSCtuopVi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the models and output function to test simple counts. Looking at people entering and leaving a room"
      ],
      "metadata": {
        "id": "4OV4o-jxpaZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6qc00oxaoH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a27c2a3-de0f-4ee1-8699-4a30a390df8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.1\n",
            "  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.1) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.1) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.1) (2024.8.30)\n",
            "Downloading transformers-4.44.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed transformers-4.44.1\n",
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "005603d3744d45dcb19137397942fd02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Collecting pandas==2.0.3\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.0.3\n",
            "Collecting langchain==0.2.14\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.14) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.14) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.14) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.14) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain==0.2.14)\n",
            "  Downloading langchain_core-0.2.39-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.14)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.14)\n",
            "  Downloading langsmith-0.1.120-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.14) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.14) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.14) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.2.14)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.14) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.14) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.14) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.14) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.14) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.14) (1.11.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.32->langchain==0.2.14)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain==0.2.14) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain==0.2.14) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.14)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.14)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.14) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.14) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.14) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.14) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.14) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.14) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.14) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.14)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.14) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.14)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain==0.2.14)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.14) (1.2.2)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.39-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.6/396.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.120-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-core-0.2.39 langchain-text-splitters-0.2.4 langsmith-0.1.120 orjson-3.10.7 tenacity-8.5.0\n",
            "Collecting accelerate==0.33.0\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\n",
            "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "Successfully installed accelerate-0.33.0\n",
            "Collecting bitsandbytes==0.43.3\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.3) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.3) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.43.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes==0.43.3) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.3\n",
            "Collecting langchain-community==0.2.12\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.12)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (0.2.39)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (0.1.120)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.12) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.12)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.12)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (0.2.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (2.9.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.2.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.2.12) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.2.12) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.2.12) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.12) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (2.23.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.12)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (1.2.2)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: pydantic==2.9.1 in /usr/local/lib/python3.10/dist-packages (2.9.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.9.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.9.1) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.9.1) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "#installing libraries\n",
        "\n",
        "#run llama 2 on computer\n",
        "#https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing\n",
        "\n",
        "#!pip uninstall transformers\n",
        "!pip install transformers==4.44.1\n",
        "!pip install numpy==1.25.2\n",
        "!pip install torch==2.4.0\n",
        "!pip install pandas==2.0.3\n",
        "!pip install langchain==0.2.14\n",
        "!pip install accelerate==0.33.0\n",
        "!pip install bitsandbytes==0.43.3\n",
        "!pip install langchain-community==0.2.12\n",
        "!pip install pydantic==2.9.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "\n",
        "#documentation: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "from pydantic import Field\n",
        "\n",
        "from typing import Optional, List\n",
        "\n",
        "import re\n",
        "import ast\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xsu_DzADbxxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing - Do Not Run Before Langchain - Memory Issue"
      ],
      "metadata": {
        "id": "WXtzeqzZekqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to load and run models\n",
        "\n",
        "#documentation: https://github.com/huggingface/transformers/issues/14498\n",
        "\n",
        "#for llama\n",
        "# access_token = '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'\n",
        "\n",
        "# def get_output(model_name, input_text, max_tokens = 700):\n",
        "#   tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)\n",
        "#   model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', load_in_4bit=True, use_auth_token=access_token)\n",
        "\n",
        "#   tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "#   input_tokens = tokenizer.encode(input_text, return_tensors='pt', padding=True, truncation=True, return_attention_mask=True).to('cuda:0')\n",
        "\n",
        "#   with torch.no_grad():\n",
        "#     output_tokens = model.generate(input_tokens, max_new_tokens = max_tokens, return_dict_in_generate = True, output_scores= True)\n",
        "\n",
        "#   created_tokens = output_tokens.sequences\n",
        "\n",
        "#   created_text = tokenizer.decode(created_tokens[0], skip_special_tokens = True)\n",
        "\n",
        "#   logits = torch.stack(output_tokens.scores, dim = 1)\n",
        "#   probabilities = torch.softmax(logits, dim = 1)\n",
        "\n",
        "#   logits_list = logits.tolist()\n",
        "#   probabilities_list = probabilities.tolist()\n",
        "\n",
        "#   return created_text, logits_list, probabilities_list"
      ],
      "metadata": {
        "id": "0m7077Z9diSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Testing"
      ],
      "metadata": {
        "id": "kJyNkzNx6k-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "\n",
        "#model_name = 'gpt2'\n",
        "#model_name = 'meta-llama/Llama-2-7b-chat-hf' #need to get access to this\n",
        "model_name = 'tiiuae/falcon-7b'\n",
        "\n",
        "#simple count\n",
        "#input_text = 'If 6 people entered a room and 2 left the room, how many people are left in the room? Give your reasoning'\n",
        "\n",
        "#complex count, uncommon numbers\n",
        "#input_text = 'There are 3567 people in a room. 2500 are males and the rest are females. 6 more males entered, 17 females left and 2 children entered. How many people are in the room?'\n",
        "\n",
        "#complex count\n",
        "#input_text = '32 men and 14 women enter a room. 6 women leave, 17 men enter and 3 children enter. How many people are in the room?'\n",
        "\n",
        "#simple count\n",
        "#input_text = '32 people enter a room. 5 leave, 15 enter, 62 enter and 27 leave. How many people are in the room?'\n",
        "\n",
        "#simple count, uncommon numbers\n",
        "#the answer is 263850\n",
        "#it gets the task approximately right\n",
        "#input_text = '322456 people enter a room. 52356 leave, 15151 enter, 6233 enter and 27634 leave. How many people are in the room?'\n",
        "\n",
        "#input_text = 'How many different words for addition are there?'\n",
        "\n",
        "input_text = \"Kendra made punch for her friend 's birthday party . She used 0.25 gallon of grape juice , 0.375 gallon of cranberry juice , and 0.125 gallon of club soda . How many gallons of punch did Kendra make ?\"\n",
        "\n",
        "created_text, logits_list, probabilities_list = get_output(model_name, input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "e32c3f1fd3ec461ba77c440add295b10",
            "f5c7a704c1324feeb5cd3f9313a9db8c",
            "b6e248aa08444d58976a67228702588b",
            "43ee5e71ea1840f99f2b023ffd8f4a6b",
            "5e83e43148f4419a87aee84aedbd92fd",
            "2ad9701f5c094bc38e9f62d4d28ffcf3",
            "e66e43ef2c3243caba5741528e9a4706",
            "ee93a87dd35147c890651d6955babf2c",
            "555e4e7706f1406d9a437d69ede203fe",
            "291ba42cf14a4f608713758e2983aeff",
            "2d9acc46f1984935bdf1bc1c386c608c"
          ]
        },
        "id": "eSjhKEcMI-Vr",
        "outputId": "b6929cec-d78a-4979-e8ee-147a1b5651e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e32c3f1fd3ec461ba77c440add295b10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "created_text = created_text.replace('\\n\\n', ' ')\n",
        "llama7b_text = created_text.replace('\\n' , ' ')\n",
        "created_text\n",
        "#llama7b_text\n",
        "\n",
        "#logits_list\n",
        "#probabilities_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MWq8TapkJYLR",
        "outputId": "270da2a5-7c69-4ba1-c9aa-62e83c71a121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Kendra made punch for her friend's birthday party. She used 0.25 gallon of grape juice, 0.375 gallon of cranberry juice, and 0.125 gallon of club soda. How many gallons of punch did Kendra make?\\n1 Answer\\nExplanation:\\nImpact of this question\\n104 views around the world\\nYou can reuse this answer\\nCreative Commons License\\nCreative Commons License\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "simple numbers are easy to accomplish which is expected as they are more likely to be seen. But uncommon numbers cause problems\n",
        "\n",
        "simple problem with simple numbers = easy\n",
        "\n",
        "simple problem with complex numbers = hard\n",
        "\n",
        "complex problem with simple numbers = hard\n",
        "\n",
        "complex problem with complex numbers = hard"
      ],
      "metadata": {
        "id": "Xg1gmPTPpo_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counter code test"
      ],
      "metadata": {
        "id": "jH-h5Juc2IpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple counter method for addition, subtraction and getting the current count\n",
        "\n",
        "class RoomCounter:\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "\n",
        "    def enter(self, number):\n",
        "        self.count = self.count + number\n",
        "        print('The number of people in the room is:')\n",
        "        return self.count\n",
        "\n",
        "    def leave(self, number):\n",
        "      if self.count - number < 0:\n",
        "          print('Oops! Not enough people that can leave the room! The number of people in the room is still:')\n",
        "          return self.count\n",
        "      else:\n",
        "          self.count = self.count - number\n",
        "          print('The number of people in the room is:')\n",
        "          return self.count\n",
        "\n",
        "#testing another way to ensure non negative output\n",
        "#    def leave(self, number):\n",
        "#        self.count = max(0, self.count - number)\n",
        "#        return self.count\n",
        "\n",
        "    def current_count(self):\n",
        "        print('The number of people in the room is:')\n",
        "        return self.count"
      ],
      "metadata": {
        "id": "1y8hRSggo_3a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = RoomCounter()"
      ],
      "metadata": {
        "id": "svIfCfvlZ7To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter.enter(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OguoQaFOZ7aJ",
        "outputId": "0b680ee2-fed3-4403-b087-dab7d6e2f9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of people in the room is:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.current_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwtPpJrRaHmh",
        "outputId": "e1cbbba1-cf4c-4f34-c3f2-842feee3c13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of people in the room is:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.leave(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP1BobOaaHpW",
        "outputId": "88f95337-1b6f-4a85-eb14-6ec8e5912ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oops! Not enough people that can leave the room! The number of people in the room is still:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.leave(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlb4KTY4aHrm",
        "outputId": "af7d1d90-9691-4244-d539-98357298aad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of people in the room is:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Tests"
      ],
      "metadata": {
        "id": "F7_nRGatyhxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(self, input_text, max_tokens=1000):\n",
        "\n",
        "  self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "  input_tokens = self.tokenizer.encode(input_text, return_tensors='pt', padding=True, truncation=True, return_attention_mask=True).to(self.device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output_tokens = self.model.generate(input_tokens, max_new_tokens=max_tokens, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "  created_tokens = output_tokens.sequences\n",
        "\n",
        "  created_text = self.tokenizer.decode(created_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "  logits = torch.stack(output_tokens.scores, dim=1)\n",
        "  logits = logits.float()\n",
        "\n",
        "  probabilities = F.softmax(logits, dim=1)\n",
        "\n",
        "  logits_list = logits.tolist()\n",
        "  probabilities_list = probabilities.tolist()\n",
        "\n",
        "  return created_text, logits_list, probabilities_list"
      ],
      "metadata": {
        "id": "cFnwMS47yeCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
        "#https://newsletter.kaitchup.com/p/run-llama-2-chat-models-on-your-computer?r=2kp66c&utm_campaign=post&utm_medium=web&triedRedirect=true\n",
        "#https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing\n",
        "\n",
        "#used the above documentation to get a quantized version of Llama 2 running\n",
        "\n",
        "#for llama\n",
        "access_token = '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'\n",
        "\n",
        "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map='auto',\n",
        "                                             load_in_8bit=True,\n",
        "                                             llm_int8_enable_fp32_cpu_offload=True,\n",
        "                                             use_auth_token=access_token)\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=1000,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.0\n",
        ")\n",
        "\n",
        "llama7b = HuggingFacePipeline(pipeline=llm_pipeline)"
      ],
      "metadata": {
        "id": "5Cwt-uElhqyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160,
          "referenced_widgets": [
            "30fa4943b3794cf1b113af99476e4b76",
            "042ea93e326340a894ca4dd03211eff1",
            "57691a5266704ea7b5bfaaa5aedfe882",
            "994fe9dfb6f24c92a125cc23754971a0",
            "8174948df6be4feba0d58677288b7592",
            "dc6a9e94e62241adaaa0ef96d7c89291",
            "ca4b6ef438414d52ad38cffaf5a7c4f6",
            "72fafebfe2434fd2847b65d69d0227ae",
            "9cb7c1098e4b41ff97ea22c1066722d1",
            "e249712556e541a9abe4f513e992f27c",
            "a7e88283da6d4df5a9a4a05ea21133e4"
          ]
        },
        "outputId": "9c3da60b-803e-43a9-8200-50dad45e6ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30fa4943b3794cf1b113af99476e4b76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_output(input_text, max_tokens=1000):\n",
        "\n",
        "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "#     input_tokens = tokenizer.encode(input_text, return_tensors='pt', padding=True, truncation=True, return_attention_mask=True).to('cuda:0')\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         output_tokens = model.generate(input_tokens, max_new_tokens=max_tokens, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "#     created_tokens = output_tokens.sequences\n",
        "\n",
        "#     created_text = tokenizer.decode(created_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "#     logits = torch.stack(output_tokens.scores, dim=1)\n",
        "#     logits = logits.float()\n",
        "\n",
        "#     probabilities = F.softmax(logits, dim=1)\n",
        "\n",
        "#     logits_list = logits.tolist()\n",
        "#     probabilities_list = probabilities.tolist()\n",
        "\n",
        "#     return created_text, logits_list, probabilities_list"
      ],
      "metadata": {
        "id": "J5c0vs-xpKR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Counter:\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "\n",
        "    def add(self, number):\n",
        "        self.count = self.count + number\n",
        "        return self.count\n",
        "\n",
        "    def subtract(self, number):\n",
        "      if self.count - number < 0:\n",
        "          return self.count\n",
        "      else:\n",
        "          self.count = self.count - number\n",
        "          return self.count\n",
        "\n",
        "    def current_count(self):\n",
        "        return self.count"
      ],
      "metadata": {
        "id": "QCMbQOUAdvj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting_prompt_template = \"\"\"\n",
        "# You are managing the number of people in a room. People can enter or leave the room.\n",
        "# Please instruct the counter to update the number of people based on the input.\n",
        "\n",
        "# The current number of people in the room is given by {count}\n",
        "\n",
        "# The user has asked: {user_input}\n",
        "\n",
        "# For each action (\"enter\" or \"leave\"), describe in detail what the counter does, how the count changes, and provide the updated count after each step.\n",
        "\n",
        "# At the end, you will provide the final count of people in the room.\n",
        "# \"\"\"\n",
        "\n",
        "# counting_prompt = PromptTemplate(\n",
        "#     input_variables=['count', 'user_input'],\n",
        "#     template=counting_prompt_template,\n",
        "# )\n",
        "\n",
        "# llama7b_counting_chain = LLMChain(llm=llama7b, prompt=counting_prompt)"
      ],
      "metadata": {
        "id": "dQGTrEVz3boT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the counting prompt is a just a front end for visualizing the extracting prompt which is doing most of the heavy lifting to identify what is added and what is subtracted\n",
        "#https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain\n",
        "\n",
        "#prompt template referenced through langchain documents\n",
        "\n",
        "counting_prompt_template = \"\"\"\n",
        "You are managing the count of various things and need to explain what is being added or subtracted and why.\n",
        "\n",
        "The user has asked: {user_input}\n",
        "\n",
        "Please explain the calculation and provide the final answer.\n",
        "\"\"\"\n",
        "\n",
        "counting_prompt = PromptTemplate(\n",
        "    input_variables=['user_input'],\n",
        "    template=counting_prompt_template,\n",
        ")\n",
        "\n",
        "llama7b_counting_chain = LLMChain(llm=llama7b, prompt=counting_prompt)"
      ],
      "metadata": {
        "id": "UmkOZGbkz3JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting_prompt_template = \"\"\"\n",
        "# You are skilled at extracting numbers from text that contains information about things entering and leaving a room.\n",
        "\n",
        "# The user has asked: {user_input}\n",
        "\n",
        "# From the question, you will extract all instances where things enter or leave the room. For each action, you will provide the number of things and whether they enter or leave.\n",
        "\n",
        "# Return the result as a list of tuples in the format [(number ,'enter' or 'leave'), (number, 'enter' or 'leave')]. Expand the list for as many extracted actions.\n",
        "# The tuples can only include a number and either the word enter or leave.\n",
        "\n",
        "# You do not need to present me with an example scenario or extra text, just focus on returning the list of tuples.\n",
        "\n",
        "# Highlight your returned result by specifying, \"My final answer is:\" and then show your results in the next line in a single line.\n",
        "# \"\"\"\n",
        "\n",
        "# extracting_prompt = PromptTemplate(\n",
        "#     input_variables=['user_input'],\n",
        "#     template=extracting_prompt_template\n",
        "# )\n",
        "\n",
        "# llama7b_extracting_chain = LLMChain(llm=llama7b, prompt=extracting_prompt)"
      ],
      "metadata": {
        "id": "lZfBvfeD1Ewz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracting_prompt_template = \"\"\"\n",
        "You are skilled at extracting numbers from text that contains information about subjects being added or subtracted. Adding and subtracting are actions.\n",
        "\n",
        "The user has asked: {user_input}\n",
        "\n",
        "Your task is to carefully extract the numbers from the what the user asks and determine whether they should be an addition action (add) or a subtraction action (subtract) based on the subject in question.\n",
        "Subjects that increase or enter should be added (add) and subjects that decrease or leave should be subtracted (subtract).\n",
        "\n",
        "Not all information in the user question will be relevant. Some numbers may or may not be used. You must think step by step about what the question is asking and then extract the related numbers that directly impact the main subject.\n",
        "If for example the question includes information on cats and dogs but simply asks for the final number of dogs then the information on cats should be ignored as the main subject is dogs.\n",
        "\n",
        "Make sure to focus on the main subject and return your result as a list of tuples in the format [(number ,'add' or 'subtract'), (number, 'add' or 'subtract')]. Expand the list for as many extracted actions.\n",
        "\n",
        "You do not need to present me with an example scenario or extra text.\n",
        "Just focus on returning the list of tuples.\n",
        "\n",
        "Highlight your returned result by specifying, \"My final answer is:\" and then show your results in the next line in a single line.\n",
        "\"\"\"\n",
        "\n",
        "extracting_prompt = PromptTemplate(\n",
        "    input_variables=['user_input'],\n",
        "    template=extracting_prompt_template\n",
        ")\n",
        "\n",
        "llama7b_extracting_chain = LLMChain(llm=llama7b, prompt=extracting_prompt)"
      ],
      "metadata": {
        "id": "QrAo86Nn1P_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_prompt_template = \"\"\"\n",
        "You are skilled at deciding whether a question is about addition/subtraction or not.\n",
        "If the quesiton is about addition/subtraction then you must then decide whether you want to use a addition/subtraction machine that can help you or whether you would like to answer the question without help.\n",
        "There can only be one decision, yes or no.\n",
        "\n",
        "The user has asked: {user_input}\n",
        "\n",
        "Given, the user question, would you like the help of an addition/subtraction machine?\n",
        "\n",
        "You do not need to provide examples or extra text.\n",
        "Just focus on returning yes or no decision and a reason for the decision\n",
        "\n",
        "Highlight your answer by specifiying, \"Would I like to use the help of an addition/subtraction machine:\" and then show your decision either yes or no\n",
        "Highlight your answer by specifiying, \"My reason is:\" and then show your reason for the decision\n",
        "\"\"\"\n",
        "decision_prompt = PromptTemplate(\n",
        "    input_variables=['user_input'],\n",
        "    template=decision_prompt_template\n",
        "    )\n",
        "\n",
        "llama7b_decision_chain = LLMChain(llm=llama7b, prompt=decision_prompt)"
      ],
      "metadata": {
        "id": "KGbHmxu9DOI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing function for extraction - this works fine, do not run the try block\n",
        "\n",
        "# def llama7b_people_extractor(user_input):\n",
        "\n",
        "#   extracted_response = llama7b_extracting_chain.run({'user_input': user_input})\n",
        "\n",
        "#   try:\n",
        "#     extracted_matches = eval(extracted_response)\n",
        "#     return extracted_matches\n",
        "#   except (SyntaxError, ValueError):\n",
        "#     #static value to check if the parsing was correct\n",
        "#     return [(0.001, 'enter')]\n",
        "\n",
        "#   return extracted_response"
      ],
      "metadata": {
        "id": "5HpD8yzK4CSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#actual extracting function using final extracting prompt\n",
        "#takes the prompts, retries a few times as it tries to parse through regex and evaluate\n",
        "#if no match is found a static value is returned\n",
        "# regex documentation: https://www.rexegg.com/regex-quickstart.php\n",
        "\n",
        "def llama7b_extractor(user_input):\n",
        "\n",
        "    max_retries = 5\n",
        "    attempt = 0\n",
        "\n",
        "    while attempt < max_retries:\n",
        "\n",
        "        created_text, logits_list, probabilities_list = llama7b_extracting_chain.run({'user_input': user_input})\n",
        "\n",
        "        matched_response = re.search(r\"My final answer is:\\s*(\\[(.*?)\\])\", created_text)\n",
        "\n",
        "        if matched_response:\n",
        "            try:\n",
        "                matched_string = matched_response.group(1).strip()\n",
        "\n",
        "                #ast literal safer than just eval - stops random function calling from my reading of stack over flow. will only check for objects like lists and strings etc\n",
        "                #https://stackoverflow.com/questions/15197673/using-pythons-eval-vs-ast-literal-eval\n",
        "                extracted_match = ast.literal_eval(matched_string)\n",
        "\n",
        "                return {\n",
        "                    'extracted_match': extracted_match,\n",
        "                    'logits': logits_list,\n",
        "                    'probabilities': probabilities_list\n",
        "                }\n",
        "            except (SyntaxError, ValueError):\n",
        "                pass\n",
        "\n",
        "        attempt += 1\n",
        "        print('Oops, no valid match, let me try again...')\n",
        "\n",
        "    #static value to check if the parsing was correct\n",
        "    #returned to keep the loop going and not cause errors\n",
        "    return {\n",
        "        'extracted_match': [(0, 'add'), (0, 'subtract')],\n",
        "        'logits': None,\n",
        "        'probabilities': None\n",
        "    }"
      ],
      "metadata": {
        "id": "-uqkPg7COd9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that runs the extractor chain, gets the output, matches the parsed list of tuples and then runs the counter class\n",
        "\n",
        "def llama7b_counter(user_input):\n",
        "\n",
        "    current_count = llama_counter.current_count()\n",
        "    updated_count = current_count\n",
        "\n",
        "    counter_matches = llama7b_extractor(user_input)\n",
        "\n",
        "    additions = []\n",
        "    subtractions = []\n",
        "\n",
        "    for match in counter_matches:\n",
        "        try:\n",
        "            number_str = re.sub(r'[^\\d.-]', '', match[0])\n",
        "            number = float(number_str)\n",
        "            action = match[1].lower()\n",
        "\n",
        "            if action == 'add':\n",
        "                additions.append(number)\n",
        "            elif action == 'subtract':\n",
        "                subtractions.append(number)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing match {match}: {e}\") #staic value to account for errors and keep the loop going\n",
        "            number = 0\n",
        "            action = 'add'\n",
        "\n",
        "            if action == 'add':\n",
        "                additions.append(number)\n",
        "            elif action == 'subtract':\n",
        "                subtractions.append(number)\n",
        "\n",
        "    for number in additions: #following bodmas rules, doing subtraction last\n",
        "        updated_count = llama_counter.add(number)\n",
        "\n",
        "    for number in subtractions:\n",
        "        updated_count = llama_counter.subtract(number)\n",
        "\n",
        "    counter_response = llama7b_counting_chain.run({'user_input': user_input})\n",
        "\n",
        "    return counter_response, updated_count, counter_matches"
      ],
      "metadata": {
        "id": "4Ip2y_ob304w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decision function\n",
        "#simsilar to the above but first the decision chain is run\n",
        "#then the parsing of whether yes or no\n",
        "#if decision is yes then the counter function runs else the output is given by a direct prompt\n",
        "\n",
        "def decide_to_count(user_input):\n",
        "\n",
        "    max_retries = 5\n",
        "    attempt = 0\n",
        "\n",
        "    decision_pattern = r'would i like to use the help of an addition/subtraction machine:\\s*(yes|no)\\s*'\n",
        "    #decision_pattern = r'would i like to use the help of a counting machine:\\s*(yes|no)\\s*'\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        decision = llama7b_decision_chain.run({'user_input': user_input}).lower()\n",
        "\n",
        "        decision_match = re.search(decision_pattern, decision)\n",
        "\n",
        "        if decision_match:\n",
        "            match_value = decision_match.group(1)\n",
        "\n",
        "            if match_value == \"yes\":\n",
        "                counter_response, updated_count, counter_matches = llama7b_counter(user_input)\n",
        "                return {\n",
        "                    'decision': decision,\n",
        "                    'counter_response': counter_response,\n",
        "                    'updated_count': updated_count,\n",
        "                    'counter_matches': counter_matches\n",
        "                }\n",
        "            elif match_value == \"no\":\n",
        "                created_text, logits_list, probabilities_list = get_output(user_input)\n",
        "                return {\n",
        "                    'decision': decision,\n",
        "                    'created_text': created_text,\n",
        "                    'logits_list': logits_list,\n",
        "                    'probabilities_list': probabilities_list\n",
        "                }\n",
        "\n",
        "        attempt += 1\n",
        "        print('Oops, no clear decision, let me try again...')\n",
        "\n",
        "    return {\n",
        "        'decision': decision,\n",
        "        'Oops': \"Unable to provide a clear yes or no after 5 attempts\"\n",
        "    }"
      ],
      "metadata": {
        "id": "1SxWkG9gEz-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#readability function just to display all the results and validate\n",
        "def display_result(result):\n",
        "    print(\"----- Decision Result -----\")\n",
        "    print(result['decision'].strip())\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "\n",
        "    if 'Oops' in result:\n",
        "        print(\"----- Oops -----\")\n",
        "        print(result['Oops'])\n",
        "        print(\"--------------------------\")\n",
        "\n",
        "    elif 'counter_response' in result:\n",
        "        print(\"----- Counter Logic -----\")\n",
        "        print(result['counter_response'].strip())\n",
        "        print(f\"Updated Count: {result['updated_count']}\")\n",
        "\n",
        "        print(\"Counter Matches:\")\n",
        "        for match in result['counter_matches']:\n",
        "            print(f\"  Value: {match[0]}, Action: {match[1]}\")\n",
        "        print(\"--------------------------\")\n",
        "\n",
        "    elif 'created_text' in result:\n",
        "        print(\"----- Created Text -----\")\n",
        "        print(result['created_text'])\n",
        "        print(\"--------------------------\")"
      ],
      "metadata": {
        "id": "InHrMH4ROefv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing questions\n",
        "\n",
        "llama_counter = Counter()\n",
        "\n",
        "#user_input = '322456 people enter a room. 52356 leave, 15151 enter, 6233 enter and 27634 leave. How many people are in the room?' #answer is 263850. does not work well - possibly the prompt specifically says people - changing the prompt to things works\n",
        "#user_input = '100 people leave the room, 6000 people leave the room and 1 person enters the room. How many people are in the room?'\n",
        "#user_input = '12346 people enter the room, 6235 people leave the room, 719246991 people enter the room and 1082350 leave the room. How many people are in the room?' #answer is 718170752\n",
        "#user_input = \"Sam has 86 yellow and 20 green marbles . Joan took 25 of Sam 's yellow marbles . How many yellow marbles does Sam now have ?\"\n",
        "#user_input = \"There are 31 oak trees and 32 orange trees currently in the park . Park workers will plant oak trees today . When the workers are finished there will be 95 oak trees in the park . How many oak trees did the workers plant today ?\"\n",
        "#user_input = \"Kendra made punch for her friend 's birthday party . She used 0.25 gallon of grape juice , 0.375 gallon of cranberry juice , and 0.125 gallon of club soda . How many gallons of punch did Kendra make ?\"\n",
        "#user_input = \"There were 32 bales of hay in the barn and 26 bales in the shed. Jason stacked bales in the barn today. There are now 98 bales of hay in the barn. How many bales did he store in the barn?\"\n",
        "#user_input = \"Dan had 14 peaches and 10 pears at his roadside fruit dish. He went to the orchard and picked peaches to stock up. There are now 85 peaches. How many did he pick?\"\n",
        "#user_input = \"What colour is the sun?\"\n",
        "#user_input = \"Joan found 70 seashells on the beach. She gave Sam some of her seashells. She has 27 seashells left. How many seashells did she give to Sam?\"\n",
        "\n",
        "#for testing the extraction\n",
        "extracted_items = llama7b_extractor(user_input)\n",
        "print(f\"Extracted Items: {extracted_items}\")\n",
        "extracted_match = extracted_items.get('extracted_match')\n",
        "logits = extracted_items.get('logits')\n",
        "\n",
        "#for the overall counter\n",
        "# counter_response, updated_count, counter_matches = llama7b_counter(user_input)\n",
        "# print(f\"LLM Response: {counter_response}\")\n",
        "# print(f\"Updated Count: {updated_count}\")\n",
        "# print(f\"Counter Matches: {counter_matches}\")\n",
        "\n",
        "#for testing decision\n",
        "#result = decide_to_count(user_input)\n",
        "#display_result(result)\n",
        "#print(result)"
      ],
      "metadata": {
        "id": "wDUTrdRT31eF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#emptying the memory to try again with different models"
      ],
      "metadata": {
        "id": "ZLA2ckxA2K3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try to get the llm to talk more about the steps being taken\n",
        "#replace the regex with an llm - this is done"
      ],
      "metadata": {
        "id": "BDg7jao4gPBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#other documentation\n",
        "\n",
        "#https://python.langchain.com/v0.2/docs/tutorials/extraction/\n",
        "#https://www.geeksforgeeks.org/python-regex-re-search-vs-re-findall/\n",
        "#https://www.youtube.com/watch?v=aywZrzNaKjs\n",
        "#https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/"
      ],
      "metadata": {
        "id": "iKtXlV5VivDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}